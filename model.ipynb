{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to use colab for gpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup and Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imghdr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 19/80 [00:00<00:00, 96.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gentrain_001.png\n",
      "gentrain_002.png\n",
      "gentrain_003.png\n",
      "gentrain_004.png\n",
      "gentrain_005.png\n",
      "gentrain_006.png\n",
      "gentrain_007.png\n",
      "gentrain_008.png\n",
      "gentrain_009.png\n",
      "gentrain_010.png\n",
      "gentrain_011.png\n",
      "gentrain_012.png\n",
      "gentrain_013.png\n",
      "gentrain_014.png\n",
      "gentrain_015.png\n",
      "gentrain_016.png\n",
      "gentrain_017.png\n",
      "gentrain_018.png\n",
      "gentrain_019.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32/80 [00:00<00:00, 107.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gentrain_020.png\n",
      "gentrain_021.png\n",
      "gentrain_022.png\n",
      "gentrain_023.png\n",
      "gentrain_024.png\n",
      "gentrain_025.png\n",
      "gentrain_026.png\n",
      "gentrain_027.png\n",
      "gentrain_028.png\n",
      "gentrain_029.png\n",
      "gentrain_030.png\n",
      "gentrain_031.png\n",
      "gentrain_032.png\n",
      "gentrain_033.png\n",
      "gentrain_034.png\n",
      "gentrain_035.png\n",
      "gentrain_036.png\n",
      "gentrain_037.png\n",
      "gentrain_038.png\n",
      "gentrain_039.png\n",
      "gentrain_040.png\n",
      "gentrain_041.png\n",
      "gentrain_042.png\n",
      "gentrain_043.png\n",
      "gentrain_044.png\n",
      "gentrain_045.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 57/80 [00:00<00:00, 109.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gentrain_046.png\n",
      "gentrain_047.png\n",
      "gentrain_048.png\n",
      "gentrain_049.png\n",
      "gentrain_050.png\n",
      "gentrain_051.png\n",
      "gentrain_052.png\n",
      "gentrain_053.png\n",
      "gentrain_054.png\n",
      "gentrain_055.png\n",
      "gentrain_056.png\n",
      "gentrain_057.png\n",
      "gentrain_058.png\n",
      "gentrain_059.png\n",
      "gentrain_060.png\n",
      "gentrain_061.png\n",
      "gentrain_062.png\n",
      "gentrain_063.png\n",
      "gentrain_064.png\n",
      "gentrain_065.png\n",
      "gentrain_066.png\n",
      "gentrain_067.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 110.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gentrain_068.png\n",
      "gentrain_069.png\n",
      "gentrain_070.png\n",
      "gentrain_071.png\n",
      "gentrain_072.png\n",
      "gentrain_073.png\n",
      "gentrain_074.png\n",
      "gentrain_075.png\n",
      "gentrain_076.png\n",
      "gentrain_077.png\n",
      "gentrain_078.png\n",
      "gentrain_079.png\n",
      "gentrain_080.png\n",
      "(526, 526, 3)\n",
      "[1.6e+06 2.5e+00 2.0e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\benli\\AppData\\Local\\Temp\\ipykernel_37396\\4196441949.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  image_list = torch.tensor(image_list)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 526, 526, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "image_list = []\n",
    "theta_list = []\n",
    "\n",
    "directory = 'trainingdataimages'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in tqdm.tqdm(os.listdir(directory)):\n",
    "    print(filename)\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        # Opens a image in RGB mode\n",
    "        image = Image.open(f)\n",
    "        data = np.asarray(image)\n",
    "        matfile = 'trainingdatavalues/' + filename[:len(filename) - 4] + '.mat'\n",
    "        mat = scipy.io.loadmat(matfile)\n",
    "        image_list.append(data)\n",
    "        theta_list.append(mat['th0'][0])\n",
    "\n",
    "print(image_list[0].shape)\n",
    "print(theta_list[0])\n",
    "image_list = torch.tensor(image_list)\n",
    "theta_list = torch.tensor(theta_list)\n",
    "\n",
    "image_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Learning_Rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m Net\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m# Change final layer to predict one value\u001b[39;00m\n\u001b[0;32m      4\u001b[0m Net \u001b[39m=\u001b[39m Net\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params\u001b[39m=\u001b[39mNet\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39mLearning_Rate) \u001b[39m# Create adam optimizer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(Net)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Learning_Rate' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # Set device GPU or CPU where the training will take place\n",
    "Net = torchvision.models.resnet18() # Load net\n",
    "Net.fc = torch.nn.Linear(in_features=512, out_features=1, bias=True) # Change final layer to predict one value\n",
    "Net = Net.to(device)\n",
    "optimizer = torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer\n",
    "\n",
    "print(Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) Loss= 1599999.8 AverageLoss 31999.995\n",
      "Saving Model0.torch\n",
      "1 ) Loss= 1599999.8 AverageLoss 63999.99\n",
      "2 ) Loss= 1599999.6 AverageLoss 95999.9825\n",
      "3 ) Loss= 1599999.6 AverageLoss 127999.975\n",
      "4 ) Loss= 1599999.5 AverageLoss 159999.965\n",
      "5 ) Loss= 1599999.5 AverageLoss 191999.955\n",
      "6 ) Loss= 1599999.4 AverageLoss 223999.9425\n",
      "7 ) Loss= 1599999.4 AverageLoss 255999.93\n",
      "8 ) Loss= 1599999.2 AverageLoss 287999.915\n",
      "9 ) Loss= 1599999.2 AverageLoss 319999.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benli\\HMEI2023-bxliu\\model.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benli/HMEI2023-bxliu/model.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m Net\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benli/HMEI2023-bxliu/model.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m Loss\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mabs(pred\u001b[39m-\u001b[39mtheta0)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/benli/HMEI2023-bxliu/model.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m Loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# Backpropogate loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benli/HMEI2023-bxliu/model.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# Apply gradient descent change to weight\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benli/HMEI2023-bxliu/model.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m AverageLoss[itr\u001b[39m%\u001b[39m\u001b[39m50\u001b[39m]\u001b[39m=\u001b[39mLoss\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39m# Save loss average\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benli\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\benli\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AverageLoss=np.zeros([50]) # Save average loss for display\n",
    "for itr in range(2000): # Training loop\n",
    "    images,theta0=LoadBatch() # Load taining batch\n",
    "    images=torch.autograd.Variable(images,requires_grad=False).to(device) \n",
    "    theta0 = torch.autograd.Variable(theta0, requires_grad=False).to(device)\n",
    " \n",
    "    pred=Net(images) # make prediction\n",
    "    Net.zero_grad()\n",
    "    Loss=torch.abs(pred-theta0).mean()\n",
    "    Loss.backward() # Backpropogate loss\n",
    "    optimizer.step() # Apply gradient descent change to weight\n",
    "    AverageLoss[itr%50]=Loss.data.cpu().numpy() # Save loss average\n",
    "    print(itr,\") Loss=\",Loss.data.cpu().numpy(),'AverageLoss',AverageLoss.mean()) # Display loss\n",
    "    if itr % 200 == 0: # Save model\n",
    "        print(\"Saving Model\" +str(itr) + \".torch\") #Save model weight\n",
    "        torch.save(Net.state_dict(),   str(itr) + \".torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
